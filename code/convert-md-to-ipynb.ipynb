{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3055d104-0f28-4d2e-8f71-d6d36da0e331",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 100 lines of the R Markdown file:\n",
      "1: ---\n",
      "2: title: \"Monitor the training process\"\n",
      "3: teaching: 120\n",
      "4: exercises: 80\n",
      "5: ---\n",
      "6: \n",
      "7: ::: questions\n",
      "8: - How do I create a neural network for a regression task?\n",
      "9: - How does optimization work?\n",
      "10: - How do I monitor the training process?\n",
      "11: - How do I detect (and avoid) overfitting?\n",
      "12: - What are common options to improve the model performance?\n",
      "13: :::\n",
      "14: \n",
      "15: ::: objectives\n",
      "16: - Explain the importance of keeping your test set clean, by validating on the validation set instead of the test set\n",
      "17: - Use the data splits to plot the training process\n",
      "18: - Explain how optimization works\n",
      "19: - Design a neural network for a regression task\n",
      "20: - Measure the performance of your deep neural network\n",
      "21: - Interpret the training plots to recognize overfitting\n",
      "22: - Use normalization as preparation step for Deep Learning\n",
      "23: - Implement basic strategies to prevent overfitting\n",
      "24: :::\n",
      "25: \n",
      "26: ::: instructor\n",
      "27: ## Copy-pasting code\n",
      "28: In this episode we first introduce a simple approach to the problem,\n",
      "29: then we iterate on that a few times to, step-by-step,\n",
      "30: working towards a more complex solution.\n",
      "31: Unfortunately this involves using the same code repeatedly over and over again,\n",
      "32: only slightly adapting it.\n",
      "33: \n",
      "34: To avoid too much typing, it can help to copy-paste code from higher up in the notebook.\n",
      "35: Be sure to make it clear where you are copying from\n",
      "36: and what you are actually changing in the copied code.\n",
      "37: It can for example help to add a comment to the lines that you added.\n",
      "38: :::\n",
      "39: \n",
      "40: In this episode we will explore how to monitor the training progress, evaluate our the model predictions and finetune the model to avoid over-fitting. For that we will use a more complicated weather data-set.\n",
      "41: \n",
      "42: ## 1. Formulate / Outline the problem: weather prediction\n",
      "43: \n",
      "44: Here we want to work with the *weather prediction dataset* (the light version) which can be\n",
      "45: [downloaded from Zenodo](https://doi.org/10.5281/zenodo.5071376).\n",
      "46: It contains daily weather observations from 11 different European cities or places through the\n",
      "47: years 2000 to 2010. For all locations the data contains the variables â€˜mean temperatureâ€™, â€˜max temperatureâ€™, and â€˜min temperatureâ€™. In addition, for multiple locations, the following variables are provided: 'cloud_cover', 'wind_speed', 'wind_gust', 'humidity', 'pressure', 'global_radiation', 'precipitation', 'sunshine', but not all of them are provided for every location. A more extensive description of the dataset including the different physical units is given in accompanying metadata file. The full dataset comprises of 10 years (3654 days) of collected weather data across Europe.\n",
      "48: \n",
      "49: ![European locations in the weather prediction dataset](fig/03_weather_prediction_dataset_map.png){alt='18 European locations in the weather prediction dataset'}\n",
      "50: \n",
      "51:  A very common task with weather data is to make a prediction about the weather sometime in the future, say the next day. In this episode, we will try to predict tomorrow's sunshine hours, a challenging-to-predict feature, using a neural network with the available weather data for one location: BASEL.\n",
      "52: \n",
      "53: ## 2. Identify inputs and outputs\n",
      "54: \n",
      "55: ### Import Dataset\n",
      "56: We will now import and explore the weather data-set:\n",
      "57: \n",
      "58: ```python\n",
      "59: import pandas as pd\n",
      "60: \n",
      "61: filename_data = \"weather_prediction_dataset_light.csv\"\n",
      "62: data = pd.read_csv(filename_data)\n",
      "63: data.head()\n",
      "64: ```\n",
      "65: \n",
      "66: | | DATE \t| MONTH | \tBASEL_cloud_cover \t| \tBASEL_humidity \t| \tBASEL_pressure\t| ... |\n",
      "67: |------:|------:|---------------:|--------------:|------------------:|------------:|------------:|\n",
      "68: |0| \t20000101 \t|1 \t|8 \t|0.89 \t|1.0286|... |\n",
      "69: |1| \t20000102 \t|1 \t|8 \t|0.87 \t|1.0318|... |\n",
      "70: |2| \t20000103 \t|1 \t|5 \t|0.81 \t|1.0314|... |\n",
      "71: |3| \t20000104 \t|1 \t|7 \t|0.79 \t|1.0262|... |\n",
      "72: |4| \t20000105 \t|1 \t|5 \t|0.90 \t|1.0246|... |\n",
      "73: \n",
      "74: \n",
      "75: ::: callout\n",
      "76: ## Load the data\n",
      "77: If you have not downloaded the data yet, you can also load it directly from Zenodo:\n",
      "78: ```python\n",
      "79: data = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1\")\n",
      "80: ```\n",
      "81: :::\n",
      "82: \n",
      "83: \n",
      "84: ### Brief exploration of the data\n",
      "85: Let us start with a quick look at the type of features that we find in the data.\n",
      "86: ```python\n",
      "87: data.columns\n",
      "88: ```\n",
      "89: \n",
      "90: ```output\n",
      "91: Index(['DATE', 'MONTH', 'BASEL_cloud_cover', 'BASEL_humidity',\n",
      "92:        'BASEL_pressure', 'BASEL_global_radiation', 'BASEL_precipitation',\n",
      "93:        'BASEL_sunshine', 'BASEL_temp_mean', 'BASEL_temp_min', 'BASEL_temp_max',\n",
      "94:         ...\n",
      "95:        'SONNBLICK_temp_min', 'SONNBLICK_temp_max', 'TOURS_humidity',\n",
      "96:        'TOURS_pressure', 'TOURS_global_radiation', 'TOURS_precipitation',\n",
      "97:        'TOURS_temp_mean', 'TOURS_temp_min', 'TOURS_temp_max'],\n",
      "98:       dtype='object')\n",
      "99: ```\n",
      "100: There is a total of 9 different measured variables (global_radiation, humidity, etcetera)\n",
      "\n",
      "\n",
      "Cells in the notebook:\n",
      "0: {'id': '5bbdc527', 'cell_type': 'markdown', 'source': '# Monitor the training process', 'metadata': {}}\n",
      "1: {'id': '516e4b50', 'cell_type': 'markdown', 'source': '\\n', 'metadata': {}}\n",
      "2: {'id': 'ebd6d935', 'cell_type': 'markdown', 'source': '### Questions\\n- How do I create a neural network for a regression task?\\n- How does optimization work?\\n- How do I monitor the training process?\\n- How do I detect (and avoid) overfitting?\\n- What are common options to improve the model performance?', 'metadata': {}}\n",
      "3: {'id': 'd6a8a76f', 'cell_type': 'markdown', 'source': '', 'metadata': {}}\n",
      "4: {'id': 'fb99e6df', 'cell_type': 'markdown', 'source': '### Objectives\\n- Explain the importance of keeping your test set clean, by validating on the validation set instead of the test set\\n- Use the data splits to plot the training process\\n- Explain how optimization works\\n- Design a neural network for a regression task\\n- Measure the performance of your deep neural network\\n- Interpret the training plots to recognize overfitting\\n- Use normalization as preparation step for Deep Learning\\n- Implement basic strategies to prevent overfitting', 'metadata': {}}\n",
      "5: {'id': '512657ab', 'cell_type': 'markdown', 'source': '\\n\\n\\nIn this episode we will explore how to monitor the training progress, evaluate our the model predictions and finetune the model to avoid over-fitting. For that we will use a more complicated weather data-set.\\n', 'metadata': {}}\n",
      "6: {'id': '2b7bad11', 'cell_type': 'markdown', 'source': '## 1. Formulate / Outline the problem: weather prediction', 'metadata': {}}\n",
      "7: {'id': '78a42081', 'cell_type': 'markdown', 'source': \"\\nHere we want to work with the *weather prediction dataset* (the light version) which can be\\n[downloaded from Zenodo](https://doi.org/10.5281/zenodo.5071376).\\nIt contains daily weather observations from 11 different European cities or places through the\\nyears 2000 to 2010. For all locations the data contains the variables â€˜mean temperatureâ€™, â€˜max temperatureâ€™, and â€˜min temperatureâ€™. In addition, for multiple locations, the following variables are provided: 'cloud_cover', 'wind_speed', 'wind_gust', 'humidity', 'pressure', 'global_radiation', 'precipitation', 'sunshine', but not all of them are provided for every location. A more extensive description of the dataset including the different physical units is given in accompanying metadata file. The full dataset comprises of 10 years (3654 days) of collected weather data across Europe.\\n\\n![European locations in the weather prediction dataset](fig/03_weather_prediction_dataset_map.png){alt='18 European locations in the weather prediction dataset'}\\n\\n A very common task with weather data is to make a prediction about the weather sometime in the future, say the next day. In this episode, we will try to predict tomorrow's sunshine hours, a challenging-to-predict feature, using a neural network with the available weather data for one location: BASEL.\\n\", 'metadata': {}}\n",
      "8: {'id': '9641f797', 'cell_type': 'markdown', 'source': '## 2. Identify inputs and outputs', 'metadata': {}}\n",
      "9: {'id': 'f087a622', 'cell_type': 'markdown', 'source': '', 'metadata': {}}\n",
      "10: {'id': '3d0a9cd8', 'cell_type': 'markdown', 'source': '### Import Dataset', 'metadata': {}}\n",
      "11: {'id': '77d9f5f8', 'cell_type': 'markdown', 'source': 'We will now import and explore the weather data-set:\\n', 'metadata': {}}\n",
      "12: {'id': 'd7bec27a', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'import pandas as pd\\n\\nfilename_data = \"weather_prediction_dataset_light.csv\"\\ndata = pd.read_csv(filename_data)\\ndata.head()', 'outputs': []}\n",
      "13: {'id': '1b1a033c', 'cell_type': 'markdown', 'source': '\\n| | DATE \\t| MONTH | \\tBASEL_cloud_cover \\t| \\tBASEL_humidity \\t| \\tBASEL_pressure\\t| ... |\\n|------:|------:|---------------:|--------------:|------------------:|------------:|------------:|\\n|0| \\t20000101 \\t|1 \\t|8 \\t|0.89 \\t|1.0286|... |\\n|1| \\t20000102 \\t|1 \\t|8 \\t|0.87 \\t|1.0318|... |\\n|2| \\t20000103 \\t|1 \\t|5 \\t|0.81 \\t|1.0314|... |\\n|3| \\t20000104 \\t|1 \\t|7 \\t|0.79 \\t|1.0262|... |\\n|4| \\t20000105 \\t|1 \\t|5 \\t|0.90 \\t|1.0246|... |\\n\\n', 'metadata': {}}\n",
      "14: {'id': '44bd4c4f', 'cell_type': 'markdown', 'source': '### Callout', 'metadata': {}}\n",
      "15: {'id': 'e104da35', 'cell_type': 'markdown', 'source': '## Load the data', 'metadata': {}}\n",
      "16: {'id': '78b8ede9', 'cell_type': 'markdown', 'source': 'If you have not downloaded the data yet, you can also load it directly from Zenodo:', 'metadata': {}}\n",
      "17: {'id': '07513af8', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'data = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1\")', 'outputs': []}\n",
      "18: {'id': '0dbbf9cd', 'cell_type': 'markdown', 'source': '\\n', 'metadata': {}}\n",
      "19: {'id': 'a4ffdd1e', 'cell_type': 'markdown', 'source': '### Brief exploration of the data', 'metadata': {}}\n",
      "20: {'id': '31dc4810', 'cell_type': 'markdown', 'source': 'Let us start with a quick look at the type of features that we find in the data.', 'metadata': {}}\n",
      "21: {'id': '6b2b939e', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'data.columns', 'outputs': []}\n",
      "22: {'id': 'ca89d361', 'cell_type': 'markdown', 'source': '', 'metadata': {}}\n",
      "23: {'id': '33db3758', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': \"Index(['DATE', 'MONTH', 'BASEL_cloud_cover', 'BASEL_humidity',\\n       'BASEL_pressure', 'BASEL_global_radiation', 'BASEL_precipitation',\\n       'BASEL_sunshine', 'BASEL_temp_mean', 'BASEL_temp_min', 'BASEL_temp_max',\\n        ...\\n       'SONNBLICK_temp_min', 'SONNBLICK_temp_max', 'TOURS_humidity',\\n       'TOURS_pressure', 'TOURS_global_radiation', 'TOURS_precipitation',\\n       'TOURS_temp_mean', 'TOURS_temp_min', 'TOURS_temp_max'],\\n      dtype='object')\", 'outputs': []}\n",
      "24: {'id': '366a2e6a', 'cell_type': 'markdown', 'source': \"There is a total of 9 different measured variables (global_radiation, humidity, etcetera)\\n\\n\\nLet's have a look at the shape of the dataset:\", 'metadata': {}}\n",
      "25: {'id': '06627082', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'data.shape', 'outputs': []}\n",
      "26: {'id': 'bd433f7c', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': '(3654, 91)', 'outputs': []}\n",
      "27: {'id': '0bf5fb89', 'cell_type': 'markdown', 'source': 'This will give both the number of samples (3654) and the number of features (89 + month +\\ndate).\\n', 'metadata': {}}\n",
      "28: {'id': '75150878', 'cell_type': 'markdown', 'source': '## 3. Prepare data', 'metadata': {}}\n",
      "29: {'id': 'bfb67b71', 'cell_type': 'markdown', 'source': '', 'metadata': {}}\n",
      "30: {'id': '228c2294', 'cell_type': 'markdown', 'source': '### Select a subset and split into data (X) and labels (y)', 'metadata': {}}\n",
      "31: {'id': '49333933', 'cell_type': 'markdown', 'source': 'The full dataset comprises of 10 years (3654 days) from which we will select only the first 3 years. The present dataset is sorted by \"DATE\", so for each row `i` in the table we can pick a corresponding feature and location from row `i+1` that we later want to predict with our model. As outlined in step 1, we would like to predict the sunshine hours for the location: BASEL.\\n', 'metadata': {}}\n",
      "32: {'id': 'a8739e19', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'nr_rows = 365*3 # 3 years\\n# data\\nX_data = data.loc[:nr_rows] # Select first 3 years\\nX_data = X_data.drop(columns=[\\'DATE\\', \\'MONTH\\']) # Drop date and month column\\n\\n# labels (sunshine hours the next day)\\ny_data = data.loc[1:(nr_rows + 1)][\"BASEL_sunshine\"]', 'outputs': []}\n",
      "33: {'id': '4e1b06f1', 'cell_type': 'markdown', 'source': \"\\nIn general, it is important to check if the data contains any unexpected values such as `9999` or `NaN` or `NoneType`. You can use the pandas `data.describe()` or `data.isnull()` function for this. If so, such values must be removed or replaced.\\nIn the present case the data is luckily well prepared and shouldn't contain such values, so that this step can be omitted.\\n\", 'metadata': {}}\n",
      "34: {'id': '9d49ee26', 'cell_type': 'markdown', 'source': '### Split data and labels into training, validation, and test set', 'metadata': {}}\n",
      "35: {'id': 'c2cec57f', 'cell_type': 'markdown', 'source': '\\nAs with classical machine learning techniques, it is required in deep learning to split off a hold-out *test set* which remains untouched during model training and tuning. It is later used to evaluate the model performance. On top, we will also split off an additional *validation set*, the reason of which will hopefully become clearer later in this lesson.\\n\\nTo make our lives a bit easier, we employ a trick to create these 3 datasets, `training set`, `test set` and `validation set`, by calling the `train_test_split` method of `scikit-learn` twice.\\n\\nFirst we create the training set and leave the remainder of 30 % of the data to the two hold-out sets.\\n', 'metadata': {}}\n",
      "36: {'id': '8bdb0d6e', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'from sklearn.model_selection import train_test_split\\n\\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_data, y_data, test_size=0.3, random_state=0)', 'outputs': []}\n",
      "37: {'id': '19bf245a', 'cell_type': 'markdown', 'source': '\\nNow we split the 30 % of the data in two equal sized parts.\\n', 'metadata': {}}\n",
      "38: {'id': '5d810c0e', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0)', 'outputs': []}\n",
      "39: {'id': 'a56772a3', 'cell_type': 'markdown', 'source': '\\nSetting the `random_state` to `0` is a short-hand at this point. Note however, that changing this seed of the pseudo-random number generator will also change the composition of your data sets. For the sake of reproducibility, this is one example of a parameters that should not change at all.\\n\\n\\n', 'metadata': {}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nbformat as nbf\n",
    "\n",
    "def rmd_to_notebook(rmd_file, notebook_file):\n",
    "    # Read the R Markdown file\n",
    "    with open(rmd_file, 'r') as file:\n",
    "        rmd_lines = file.readlines()\n",
    "    \n",
    "    # Preview the first 100 lines of the R Markdown file\n",
    "    print(\"Preview of the first 100 lines of the R Markdown file:\")\n",
    "    for i, line in enumerate(rmd_lines[:100]):\n",
    "        print(f\"{i+1}: {line}\", end='')\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    rmd_content = ''.join(rmd_lines)\n",
    "    \n",
    "    # Initialize a new notebook\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cells = []\n",
    "\n",
    "    # Extract the title from the YAML front matter\n",
    "    title_match = re.search(r'^---\\s*title:\\s*\"(.*?)\".*?---', rmd_content, flags=re.DOTALL | re.MULTILINE)\n",
    "    if title_match:\n",
    "        title = title_match.group(1)\n",
    "        cells.append(nbf.v4.new_markdown_cell(f\"# {title}\"))\n",
    "    \n",
    "    # Remove the YAML front matter\n",
    "    rmd_content = re.sub(r'^---.*?---', '', rmd_content, flags=re.DOTALL | re.MULTILINE)\n",
    "    \n",
    "    # Remove instructor notes and solutions\n",
    "    rmd_content = re.sub(r':::\\s*(instructor|solution)\\s*.*?:::', '', rmd_content, flags=re.DOTALL)\n",
    "\n",
    "    # Define regex patterns\n",
    "    header_pattern = re.compile(r'^(#+) (.*)', re.MULTILINE)\n",
    "    block_start_pattern = re.compile(r'^::+(\\s*\\w+)?')\n",
    "    block_end_pattern = re.compile(r'^::+')\n",
    "    \n",
    "    # Split the content into lines\n",
    "    lines = rmd_content.split('\\n')\n",
    "    \n",
    "    code_buffer = []\n",
    "    is_in_code_block = False\n",
    "    text_buffer = []\n",
    "    is_in_special_block = False\n",
    "    special_block_type = \"\"\n",
    "\n",
    "    def process_buffer(buffer):\n",
    "        if buffer:\n",
    "            cells.append(nbf.v4.new_markdown_cell('\\n'.join(buffer)))\n",
    "            buffer.clear()\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('```'):\n",
    "            is_in_code_block = not is_in_code_block\n",
    "            if is_in_code_block:\n",
    "                process_buffer(text_buffer)\n",
    "            else:\n",
    "                if code_buffer:\n",
    "                    cells.append(nbf.v4.new_code_cell('\\n'.join(code_buffer).strip()))\n",
    "                    code_buffer = []\n",
    "        elif is_in_code_block:\n",
    "            code_buffer.append(line)\n",
    "        else:\n",
    "            header_match = header_pattern.match(line)\n",
    "            block_start_match = block_start_pattern.match(line)\n",
    "            block_end_match = block_end_pattern.match(line)\n",
    "\n",
    "            if header_match:\n",
    "                process_buffer(text_buffer)\n",
    "                header_level = len(header_match.group(1))\n",
    "                header_text = header_match.group(2)\n",
    "                cells.append(nbf.v4.new_markdown_cell(f\"{'#' * header_level} {header_text}\"))\n",
    "            elif block_start_match:\n",
    "                process_buffer(text_buffer)\n",
    "                if block_start_match.group(1):\n",
    "                    is_in_special_block = True\n",
    "                    special_block_type = block_start_match.group(1).strip().capitalize()\n",
    "                    text_buffer.append(f\"### {special_block_type}\")\n",
    "            elif block_end_match and is_in_special_block:\n",
    "                process_buffer(text_buffer)\n",
    "                is_in_special_block = False\n",
    "            elif not block_end_match:  # Ignore lines with only colons\n",
    "                text_buffer.append(line)\n",
    "    \n",
    "    process_buffer(text_buffer)\n",
    "\n",
    "    # Print the cells\n",
    "    print(\"Cells in the notebook:\")\n",
    "    for cell_ct, cell in enumerate(cells[:40]):\n",
    "        print(f\"{cell_ct}: {cell}\")\n",
    "        \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Add cells to the notebook\n",
    "    nb['cells'] = cells\n",
    "    \n",
    "    # Write the notebook to a file\n",
    "    with open(notebook_file, 'w') as file:\n",
    "        nbf.write(nb, file)\n",
    "\n",
    "# Use the function to convert an R Markdown file to a Jupyter notebook\n",
    "rmd_to_notebook('test-markdown-to-jupyter.Rmd', 'test-markdown-to-jupyter.ipynb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314346e3-69c2-4236-85aa-21494154ecb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
