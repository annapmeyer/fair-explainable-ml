{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3055d104-0f28-4d2e-8f71-d6d36da0e331",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 100 lines of the R Markdown file:\n",
      "1: ---\n",
      "2: title: \"Monitor the training process\"\n",
      "3: teaching: 120\n",
      "4: exercises: 80\n",
      "5: ---\n",
      "6: \n",
      "7: ::: questions\n",
      "8: - How do I create a neural network for a regression task?\n",
      "9: - How does optimization work?\n",
      "10: - How do I monitor the training process?\n",
      "11: - How do I detect (and avoid) overfitting?\n",
      "12: - What are common options to improve the model performance?\n",
      "13: :::\n",
      "14: \n",
      "15: ::: objectives\n",
      "16: - Explain the importance of keeping your test set clean, by validating on the validation set instead of the test set\n",
      "17: - Use the data splits to plot the training process\n",
      "18: - Explain how optimization works\n",
      "19: - Design a neural network for a regression task\n",
      "20: - Measure the performance of your deep neural network\n",
      "21: - Interpret the training plots to recognize overfitting\n",
      "22: - Use normalization as preparation step for deep learning\n",
      "23: - Implement basic strategies to prevent overfitting\n",
      "24: :::\n",
      "25: \n",
      "26: ::: instructor\n",
      "27: ## Copy-pasting code\n",
      "28: In this episode we first introduce a simple approach to the problem,\n",
      "29: then we iterate on that a few times to, step-by-step,\n",
      "30: working towards a more complex solution.\n",
      "31: Unfortunately this involves using the same code repeatedly over and over again,\n",
      "32: only slightly adapting it.\n",
      "33: \n",
      "34: To avoid too much typing, it can help to copy-paste code from higher up in the notebook.\n",
      "35: Be sure to make it clear where you are copying from\n",
      "36: and what you are actually changing in the copied code.\n",
      "37: It can for example help to add a comment to the lines that you added.\n",
      "38: :::\n",
      "39: \n",
      "40: In this episode we will explore how to monitor the training progress, evaluate our the model predictions and finetune the model to avoid over-fitting. For that we will use a more complicated weather data-set.\n",
      "41: \n",
      "42: ## 1. Formulate / Outline the problem: weather prediction\n",
      "43: \n",
      "44: Here we want to work with the *weather prediction dataset* (the light version) which can be\n",
      "45: [downloaded from Zenodo](https://doi.org/10.5281/zenodo.5071376).\n",
      "46: It contains daily weather observations from 11 different European cities or places through the\n",
      "47: years 2000 to 2010. For all locations the data contains the variables â€˜mean temperatureâ€™, â€˜max temperatureâ€™, and â€˜min temperatureâ€™. In addition, for multiple locations, the following variables are provided: 'cloud_cover', 'wind_speed', 'wind_gust', 'humidity', 'pressure', 'global_radiation', 'precipitation', 'sunshine', but not all of them are provided for every location. A more extensive description of the dataset including the different physical units is given in accompanying metadata file. The full dataset comprises of 10 years (3654 days) of collected weather data across Europe.\n",
      "48: \n",
      "49: ![European locations in the weather prediction dataset](fig/03_weather_prediction_dataset_map.png){alt='18 European locations in the weather prediction dataset'}\n",
      "50: \n",
      "51:  A very common task with weather data is to make a prediction about the weather sometime in the future, say the next day. In this episode, we will try to predict tomorrow's sunshine hours, a challenging-to-predict feature, using a neural network with the available weather data for one location: BASEL.\n",
      "52: \n",
      "53: ## 2. Identify inputs and outputs\n",
      "54: \n",
      "55: ### Import Dataset\n",
      "56: We will now import and explore the weather data-set:\n",
      "57: \n",
      "58: ```python\n",
      "59: import pandas as pd\n",
      "60: \n",
      "61: filename_data = \"weather_prediction_dataset_light.csv\"\n",
      "62: data = pd.read_csv(filename_data)\n",
      "63: data.head()\n",
      "64: ```\n",
      "65: \n",
      "66: | | DATE \t| MONTH | \tBASEL_cloud_cover \t| \tBASEL_humidity \t| \tBASEL_pressure\t| ... |\n",
      "67: |------:|------:|---------------:|--------------:|------------------:|------------:|------------:|\n",
      "68: |0| \t20000101 \t|1 \t|8 \t|0.89 \t|1.0286|... |\n",
      "69: |1| \t20000102 \t|1 \t|8 \t|0.87 \t|1.0318|... |\n",
      "70: |2| \t20000103 \t|1 \t|5 \t|0.81 \t|1.0314|... |\n",
      "71: |3| \t20000104 \t|1 \t|7 \t|0.79 \t|1.0262|... |\n",
      "72: |4| \t20000105 \t|1 \t|5 \t|0.90 \t|1.0246|... |\n",
      "73: \n",
      "74: \n",
      "75: ::: callout\n",
      "76: ## Load the data\n",
      "77: If you have not downloaded the data yet, you can also load it directly from Zenodo:\n",
      "78: ```python\n",
      "79: data = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1\")\n",
      "80: ```\n",
      "81: :::\n",
      "82: \n",
      "83: \n",
      "84: ### Brief exploration of the data\n",
      "85: Let us start with a quick look at the type of features that we find in the data.\n",
      "86: ```python\n",
      "87: data.columns\n",
      "88: ```\n",
      "89: \n",
      "90: ```output\n",
      "91: Index(['DATE', 'MONTH', 'BASEL_cloud_cover', 'BASEL_humidity',\n",
      "92:        'BASEL_pressure', 'BASEL_global_radiation', 'BASEL_precipitation',\n",
      "93:        'BASEL_sunshine', 'BASEL_temp_mean', 'BASEL_temp_min', 'BASEL_temp_max',\n",
      "94:         ...\n",
      "95:        'SONNBLICK_temp_min', 'SONNBLICK_temp_max', 'TOURS_humidity',\n",
      "96:        'TOURS_pressure', 'TOURS_global_radiation', 'TOURS_precipitation',\n",
      "97:        'TOURS_temp_mean', 'TOURS_temp_min', 'TOURS_temp_max'],\n",
      "98:       dtype='object')\n",
      "99: ```\n",
      "100: There is a total of 9 different measured variables (global_radiation, humidity, etcetera)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-15d195070860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;31m# file_in = '4-advanced-layer-types'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;31m# file_in = '5-transfer-learning'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m \u001b[0mrmd_to_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_in\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.Rmd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_in\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.ipynb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_image_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-15d195070860>\u001b[0m in \u001b[0;36mrmd_to_notebook\u001b[1;34m(rmd_file, notebook_file, base_image_url)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mbase_image_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_image_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/tree/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/raw/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mrmd_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'!\\[(.*?)\\]\\((.*?)\\)\\{(.*?)\\}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace_image_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmd_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Define regex patterns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-15d195070860>\u001b[0m in \u001b[0;36mreplace_image_path\u001b[1;34m(match)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mweb_image_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{base_image_url}/{image_path.split('/')[-1]}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0malt_header\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"#### {alt_text.strip('{}').split('=')[1].strip()}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34mf\"![{alt_text.split('=')[1].strip('{}')} ]({web_image_url})\\n\\n{alt_header}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nbformat as nbf\n",
    "\n",
    "def rmd_to_notebook(rmd_file, notebook_file, base_image_url):\n",
    "    # Read the R Markdown file\n",
    "    with open(rmd_file, 'r') as file:\n",
    "        rmd_lines = file.readlines()\n",
    "    \n",
    "    # Preview the first 100 lines of the R Markdown file\n",
    "    print(\"Preview of the first 100 lines of the R Markdown file:\")\n",
    "    for i, line in enumerate(rmd_lines[:100]):\n",
    "        print(f\"{i+1}: {line}\", end='')\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    rmd_content = ''.join(rmd_lines)\n",
    "    \n",
    "    # Initialize a new notebook\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cells = []\n",
    "\n",
    "    # Extract the title from the YAML front matter\n",
    "    # YAML front matter starts and ends with '---' and contains title information\n",
    "    title_match = re.search(r'^---\\s*title:\\s*\"(.*?)\".*?---', rmd_content, flags=re.DOTALL | re.MULTILINE)\n",
    "    if title_match:\n",
    "        title = title_match.group(1)\n",
    "        cells.append(nbf.v4.new_markdown_cell(f\"# {title}\"))\n",
    "    \n",
    "    # Remove the YAML front matter\n",
    "    rmd_content = re.sub(r'^---.*?---', '', rmd_content, flags=re.DOTALL | re.MULTILINE)\n",
    "    \n",
    "    # Remove instructor notes and solutions\n",
    "    # These are wrapped with ::: instructor or ::: solution\n",
    "    rmd_content = re.sub(r':::\\s*(instructor|solution)\\s*.*?:::', '', rmd_content, flags=re.DOTALL)\n",
    "\n",
    "    # Replace local image paths with GitHub raw URLs and handle alt text\n",
    "    def replace_image_path(match):\n",
    "        alt_text = match.group(1)\n",
    "        image_path = match.group(2)\n",
    "        web_image_url = f\"{base_image_url}/{image_path.split('/')[-1]}\"\n",
    "        alt_header = f\"#### {alt_text.strip('{}').split('=')[1].strip()}\"\n",
    "        return f\"![{alt_text.split('=')[1].strip('{}')} ]({web_image_url})\\n\\n{alt_header}\"\n",
    "\n",
    "    # Update the base image URL to point to the raw content\n",
    "    base_image_url = base_image_url.replace('/tree/', '/raw/')\n",
    "    \n",
    "    rmd_content = re.sub(r'!\\[(.*?)\\]\\((.*?)\\)\\{(.*?)\\}', replace_image_path, rmd_content)\n",
    "\n",
    "    # Define regex patterns\n",
    "    # Header pattern matches Markdown headers (e.g., # Header, ## Subheader)\n",
    "    header_pattern = re.compile(r'^(#+) (.*)', re.MULTILINE)\n",
    "    # Block start pattern matches lines starting with ':::', optionally followed by a word\n",
    "    block_start_pattern = re.compile(r'^::+(\\s*\\w+)?')\n",
    "    # Block end pattern matches lines starting with ':::'\n",
    "    block_end_pattern = re.compile(r'^::+')\n",
    "    \n",
    "    # Split the content into lines\n",
    "    lines = rmd_content.split('\\n')\n",
    "    \n",
    "    code_buffer = []\n",
    "    is_in_code_block = False\n",
    "    text_buffer = []\n",
    "    is_in_special_block = False\n",
    "    special_block_type = \"\"\n",
    "\n",
    "    def process_buffer(buffer):\n",
    "        if buffer:\n",
    "            cells.append(nbf.v4.new_markdown_cell('\\n'.join(buffer)))\n",
    "            buffer.clear()\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('```'):\n",
    "            # Toggle the code block state\n",
    "            is_in_code_block = not is_in_code_block\n",
    "            if is_in_code_block:\n",
    "                # Process text buffer when entering a code block\n",
    "                process_buffer(text_buffer)\n",
    "            else:\n",
    "                # Add code cell when exiting a code block\n",
    "                if code_buffer:\n",
    "                    cells.append(nbf.v4.new_code_cell('\\n'.join(code_buffer).strip()))\n",
    "                    code_buffer = []\n",
    "        elif is_in_code_block:\n",
    "            # Collect lines within a code block\n",
    "            code_buffer.append(line)\n",
    "        else:\n",
    "            header_match = header_pattern.match(line)\n",
    "            block_start_match = block_start_pattern.match(line)\n",
    "            block_end_match = block_end_pattern.match(line)\n",
    "\n",
    "            if header_match:\n",
    "                # Process text buffer when encountering a header\n",
    "                process_buffer(text_buffer)\n",
    "                header_level = len(header_match.group(1))\n",
    "                header_text = header_match.group(2)\n",
    "                cells.append(nbf.v4.new_markdown_cell(f\"{'#' * header_level} {header_text}\"))\n",
    "            elif block_start_match:\n",
    "                # Process text buffer when encountering a block start\n",
    "                process_buffer(text_buffer)\n",
    "                if block_start_match.group(1):\n",
    "                    is_in_special_block = True\n",
    "                    special_block_type = block_start_match.group(1).strip().capitalize()\n",
    "                    text_buffer.append(f\"### {special_block_type}\")\n",
    "            elif block_end_match and is_in_special_block:\n",
    "                # Process text buffer when encountering a block end\n",
    "                process_buffer(text_buffer)\n",
    "                is_in_special_block = False\n",
    "            elif not block_end_match:  # Ignore lines with only colons\n",
    "                text_buffer.append(line)\n",
    "    \n",
    "    # Process any remaining text in the buffer\n",
    "    process_buffer(text_buffer)\n",
    "\n",
    "    # Remove empty cells\n",
    "    cells = [cell for cell in cells if cell['source'].strip()]\n",
    "\n",
    "    # Print the cells\n",
    "    print(\"Cells in the notebook:\")\n",
    "    for cell_ct, cell in enumerate(cells[:40]):\n",
    "        print(f\"{cell_ct}: {cell}\")\n",
    "        \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Add cells to the notebook\n",
    "    nb['cells'] = cells\n",
    "    \n",
    "    # Write the notebook to a file\n",
    "    with open(notebook_file, 'w') as file:\n",
    "        nbf.write(nb, file)\n",
    "\n",
    "# Use the function to convert an R Markdown file to a Jupyter notebook\n",
    "input_dir = '../episodes/' \n",
    "base_image_url = 'https://github.com/carpentries-incubator/deep-learning-intro/raw/main/episodes/fig'# file_in = '2-keras'\n",
    "file_in = '3-monitor-the-model'\n",
    "# file_in = '4-advanced-layer-types'\n",
    "# file_in = '5-transfer-learning'\n",
    "rmd_to_notebook(input_dir+file_in+'.Rmd', input_dir+file_in+'.ipynb', base_image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314346e3-69c2-4236-85aa-21494154ecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Endemann\\\\Documents\\\\GitHub\\\\deep-learning-intro\\\\code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8dfb68c-aca3-4204-a0e9-53c1289cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 100 lines of the R Markdown file:\n",
      "1: ---\n",
      "2: title: \"Monitor the training process\"\n",
      "3: teaching: 120\n",
      "4: exercises: 80\n",
      "5: ---\n",
      "6: \n",
      "7: ::: questions\n",
      "8: - How do I create a neural network for a regression task?\n",
      "9: - How does optimization work?\n",
      "10: - How do I monitor the training process?\n",
      "11: - How do I detect (and avoid) overfitting?\n",
      "12: - What are common options to improve the model performance?\n",
      "13: :::\n",
      "14: \n",
      "15: ::: objectives\n",
      "16: - Explain the importance of keeping your test set clean, by validating on the validation set instead of the test set\n",
      "17: - Use the data splits to plot the training process\n",
      "18: - Explain how optimization works\n",
      "19: - Design a neural network for a regression task\n",
      "20: - Measure the performance of your deep neural network\n",
      "21: - Interpret the training plots to recognize overfitting\n",
      "22: - Use normalization as preparation step for deep learning\n",
      "23: - Implement basic strategies to prevent overfitting\n",
      "24: :::\n",
      "25: \n",
      "26: ::: instructor\n",
      "27: ## Copy-pasting code\n",
      "28: In this episode we first introduce a simple approach to the problem,\n",
      "29: then we iterate on that a few times to, step-by-step,\n",
      "30: working towards a more complex solution.\n",
      "31: Unfortunately this involves using the same code repeatedly over and over again,\n",
      "32: only slightly adapting it.\n",
      "33: \n",
      "34: To avoid too much typing, it can help to copy-paste code from higher up in the notebook.\n",
      "35: Be sure to make it clear where you are copying from\n",
      "36: and what you are actually changing in the copied code.\n",
      "37: It can for example help to add a comment to the lines that you added.\n",
      "38: :::\n",
      "39: \n",
      "40: In this episode we will explore how to monitor the training progress, evaluate our the model predictions and finetune the model to avoid over-fitting. For that we will use a more complicated weather data-set.\n",
      "41: \n",
      "42: ## 1. Formulate / Outline the problem: weather prediction\n",
      "43: \n",
      "44: Here we want to work with the *weather prediction dataset* (the light version) which can be\n",
      "45: [downloaded from Zenodo](https://doi.org/10.5281/zenodo.5071376).\n",
      "46: It contains daily weather observations from 11 different European cities or places through the\n",
      "47: years 2000 to 2010. For all locations the data contains the variables â€˜mean temperatureâ€™, â€˜max temperatureâ€™, and â€˜min temperatureâ€™. In addition, for multiple locations, the following variables are provided: 'cloud_cover', 'wind_speed', 'wind_gust', 'humidity', 'pressure', 'global_radiation', 'precipitation', 'sunshine', but not all of them are provided for every location. A more extensive description of the dataset including the different physical units is given in accompanying metadata file. The full dataset comprises of 10 years (3654 days) of collected weather data across Europe.\n",
      "48: \n",
      "49: ![European locations in the weather prediction dataset](fig/03_weather_prediction_dataset_map.png){alt='18 European locations in the weather prediction dataset'}\n",
      "50: \n",
      "51:  A very common task with weather data is to make a prediction about the weather sometime in the future, say the next day. In this episode, we will try to predict tomorrow's sunshine hours, a challenging-to-predict feature, using a neural network with the available weather data for one location: BASEL.\n",
      "52: \n",
      "53: ## 2. Identify inputs and outputs\n",
      "54: \n",
      "55: ### Import Dataset\n",
      "56: We will now import and explore the weather data-set:\n",
      "57: \n",
      "58: ```python\n",
      "59: import pandas as pd\n",
      "60: \n",
      "61: filename_data = \"weather_prediction_dataset_light.csv\"\n",
      "62: data = pd.read_csv(filename_data)\n",
      "63: data.head()\n",
      "64: ```\n",
      "65: \n",
      "66: | | DATE \t| MONTH | \tBASEL_cloud_cover \t| \tBASEL_humidity \t| \tBASEL_pressure\t| ... |\n",
      "67: |------:|------:|---------------:|--------------:|------------------:|------------:|------------:|\n",
      "68: |0| \t20000101 \t|1 \t|8 \t|0.89 \t|1.0286|... |\n",
      "69: |1| \t20000102 \t|1 \t|8 \t|0.87 \t|1.0318|... |\n",
      "70: |2| \t20000103 \t|1 \t|5 \t|0.81 \t|1.0314|... |\n",
      "71: |3| \t20000104 \t|1 \t|7 \t|0.79 \t|1.0262|... |\n",
      "72: |4| \t20000105 \t|1 \t|5 \t|0.90 \t|1.0246|... |\n",
      "73: \n",
      "74: \n",
      "75: ::: callout\n",
      "76: ## Load the data\n",
      "77: If you have not downloaded the data yet, you can also load it directly from Zenodo:\n",
      "78: ```python\n",
      "79: data = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1\")\n",
      "80: ```\n",
      "81: :::\n",
      "82: \n",
      "83: \n",
      "84: ### Brief exploration of the data\n",
      "85: Let us start with a quick look at the type of features that we find in the data.\n",
      "86: ```python\n",
      "87: data.columns\n",
      "88: ```\n",
      "89: \n",
      "90: ```output\n",
      "91: Index(['DATE', 'MONTH', 'BASEL_cloud_cover', 'BASEL_humidity',\n",
      "92:        'BASEL_pressure', 'BASEL_global_radiation', 'BASEL_precipitation',\n",
      "93:        'BASEL_sunshine', 'BASEL_temp_mean', 'BASEL_temp_min', 'BASEL_temp_max',\n",
      "94:         ...\n",
      "95:        'SONNBLICK_temp_min', 'SONNBLICK_temp_max', 'TOURS_humidity',\n",
      "96:        'TOURS_pressure', 'TOURS_global_radiation', 'TOURS_precipitation',\n",
      "97:        'TOURS_temp_mean', 'TOURS_temp_min', 'TOURS_temp_max'],\n",
      "98:       dtype='object')\n",
      "99: ```\n",
      "100: There is a total of 9 different measured variables (global_radiation, humidity, etcetera)\n",
      "\n",
      "\n",
      "Cells in the notebook:\n",
      "0: {'id': '49863bdc', 'cell_type': 'markdown', 'source': '# Monitor the training process', 'metadata': {}}\n",
      "1: {'id': '3f4ef477', 'cell_type': 'markdown', 'source': '### Questions\\n- How do I create a neural network for a regression task?\\n- How does optimization work?\\n- How do I monitor the training process?\\n- How do I detect (and avoid) overfitting?\\n- What are common options to improve the model performance?', 'metadata': {}}\n",
      "2: {'id': 'cd30c587', 'cell_type': 'markdown', 'source': '### Objectives\\n- Explain the importance of keeping your test set clean, by validating on the validation set instead of the test set\\n- Use the data splits to plot the training process\\n- Explain how optimization works\\n- Design a neural network for a regression task\\n- Measure the performance of your deep neural network\\n- Interpret the training plots to recognize overfitting\\n- Use normalization as preparation step for deep learning\\n- Implement basic strategies to prevent overfitting', 'metadata': {}}\n",
      "3: {'id': '1b935fd0', 'cell_type': 'markdown', 'source': '\\n\\n\\nIn this episode we will explore how to monitor the training progress, evaluate our the model predictions and finetune the model to avoid over-fitting. For that we will use a more complicated weather data-set.\\n', 'metadata': {}}\n",
      "4: {'id': 'c4962b94', 'cell_type': 'markdown', 'source': '## 1. Formulate / Outline the problem: weather prediction', 'metadata': {}}\n",
      "5: {'id': 'd235b0aa', 'cell_type': 'markdown', 'source': \"\\nHere we want to work with the *weather prediction dataset* (the light version) which can be\\n[downloaded from Zenodo](https://doi.org/10.5281/zenodo.5071376).\\nIt contains daily weather observations from 11 different European cities or places through the\\nyears 2000 to 2010. For all locations the data contains the variables â€˜mean temperatureâ€™, â€˜max temperatureâ€™, and â€˜min temperatureâ€™. In addition, for multiple locations, the following variables are provided: 'cloud_cover', 'wind_speed', 'wind_gust', 'humidity', 'pressure', 'global_radiation', 'precipitation', 'sunshine', but not all of them are provided for every location. A more extensive description of the dataset including the different physical units is given in accompanying metadata file. The full dataset comprises of 10 years (3654 days) of collected weather data across Europe.\\n\\n![alt='18 European locations in the weather prediction dataset'](https://github.com/carpentries-incubator/deep-learning-intro/raw/main/episodes/fig/03_weather_prediction_dataset_map.png)\\n\", 'metadata': {}}\n",
      "6: {'id': 'decb6769', 'cell_type': 'markdown', 'source': \"#### '18 European locations in the weather prediction dataset'\", 'metadata': {}}\n",
      "7: {'id': 'f3515128', 'cell_type': 'markdown', 'source': \"\\n A very common task with weather data is to make a prediction about the weather sometime in the future, say the next day. In this episode, we will try to predict tomorrow's sunshine hours, a challenging-to-predict feature, using a neural network with the available weather data for one location: BASEL.\\n\", 'metadata': {}}\n",
      "8: {'id': '55ab8869', 'cell_type': 'markdown', 'source': '## 2. Identify inputs and outputs', 'metadata': {}}\n",
      "9: {'id': '2cf13497', 'cell_type': 'markdown', 'source': '### Import Dataset', 'metadata': {}}\n",
      "10: {'id': 'a5adca34', 'cell_type': 'markdown', 'source': 'We will now import and explore the weather data-set:\\n', 'metadata': {}}\n",
      "11: {'id': 'dd77786c', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'import pandas as pd\\n\\nfilename_data = \"weather_prediction_dataset_light.csv\"\\ndata = pd.read_csv(filename_data)\\ndata.head()', 'outputs': []}\n",
      "12: {'id': '06625909', 'cell_type': 'markdown', 'source': '\\n| | DATE \\t| MONTH | \\tBASEL_cloud_cover \\t| \\tBASEL_humidity \\t| \\tBASEL_pressure\\t| ... |\\n|------:|------:|---------------:|--------------:|------------------:|------------:|------------:|\\n|0| \\t20000101 \\t|1 \\t|8 \\t|0.89 \\t|1.0286|... |\\n|1| \\t20000102 \\t|1 \\t|8 \\t|0.87 \\t|1.0318|... |\\n|2| \\t20000103 \\t|1 \\t|5 \\t|0.81 \\t|1.0314|... |\\n|3| \\t20000104 \\t|1 \\t|7 \\t|0.79 \\t|1.0262|... |\\n|4| \\t20000105 \\t|1 \\t|5 \\t|0.90 \\t|1.0246|... |\\n\\n', 'metadata': {}}\n",
      "13: {'id': '725ee7ac', 'cell_type': 'markdown', 'source': '### Callout', 'metadata': {}}\n",
      "14: {'id': '34633301', 'cell_type': 'markdown', 'source': '## Load the data', 'metadata': {}}\n",
      "15: {'id': '2dd56f8f', 'cell_type': 'markdown', 'source': 'If you have not downloaded the data yet, you can also load it directly from Zenodo:', 'metadata': {}}\n",
      "16: {'id': '6a29b3fa', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'data = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1\")', 'outputs': []}\n",
      "17: {'id': '5ab0af7f', 'cell_type': 'markdown', 'source': '### Brief exploration of the data', 'metadata': {}}\n",
      "18: {'id': 'ba2efbb9', 'cell_type': 'markdown', 'source': 'Let us start with a quick look at the type of features that we find in the data.', 'metadata': {}}\n",
      "19: {'id': '5a51ad20', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'data.columns', 'outputs': []}\n",
      "20: {'id': '77d64eeb', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': \"Index(['DATE', 'MONTH', 'BASEL_cloud_cover', 'BASEL_humidity',\\n       'BASEL_pressure', 'BASEL_global_radiation', 'BASEL_precipitation',\\n       'BASEL_sunshine', 'BASEL_temp_mean', 'BASEL_temp_min', 'BASEL_temp_max',\\n        ...\\n       'SONNBLICK_temp_min', 'SONNBLICK_temp_max', 'TOURS_humidity',\\n       'TOURS_pressure', 'TOURS_global_radiation', 'TOURS_precipitation',\\n       'TOURS_temp_mean', 'TOURS_temp_min', 'TOURS_temp_max'],\\n      dtype='object')\", 'outputs': []}\n",
      "21: {'id': 'ef4289eb', 'cell_type': 'markdown', 'source': \"There is a total of 9 different measured variables (global_radiation, humidity, etcetera)\\n\\n\\nLet's have a look at the shape of the dataset:\", 'metadata': {}}\n",
      "22: {'id': '2ae55621', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'data.shape', 'outputs': []}\n",
      "23: {'id': '3d2ec650', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': '(3654, 91)', 'outputs': []}\n",
      "24: {'id': '9851ee96', 'cell_type': 'markdown', 'source': 'This will give both the number of samples (3654) and the number of features (89 + month +\\ndate).\\n', 'metadata': {}}\n",
      "25: {'id': '25dc2a78', 'cell_type': 'markdown', 'source': '## 3. Prepare data', 'metadata': {}}\n",
      "26: {'id': '4345dd67', 'cell_type': 'markdown', 'source': '### Select a subset and split into data (X) and labels (y)', 'metadata': {}}\n",
      "27: {'id': '54b3abb6', 'cell_type': 'markdown', 'source': 'The full dataset comprises of 10 years (3654 days) from which we will select only the first 3 years. The present dataset is sorted by \"DATE\", so for each row `i` in the table we can pick a corresponding feature and location from row `i+1` that we later want to predict with our model. As outlined in step 1, we would like to predict the sunshine hours for the location: BASEL.\\n', 'metadata': {}}\n",
      "28: {'id': 'a90b64be', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'nr_rows = 365*3 # 3 years\\n# data\\nX_data = data.loc[:nr_rows] # Select first 3 years\\nX_data = X_data.drop(columns=[\\'DATE\\', \\'MONTH\\']) # Drop date and month column\\n\\n# labels (sunshine hours the next day)\\ny_data = data.loc[1:(nr_rows + 1)][\"BASEL_sunshine\"]', 'outputs': []}\n",
      "29: {'id': '7c7d6675', 'cell_type': 'markdown', 'source': \"\\nIn general, it is important to check if the data contains any unexpected values such as `9999` or `NaN` or `NoneType`. You can use the pandas `data.describe()` or `data.isnull()` function for this. If so, such values must be removed or replaced.\\nIn the present case the data is luckily well prepared and shouldn't contain such values, so that this step can be omitted.\\n\", 'metadata': {}}\n",
      "30: {'id': '5cb5576e', 'cell_type': 'markdown', 'source': '### Split data and labels into training, validation, and test set', 'metadata': {}}\n",
      "31: {'id': '15d59da4', 'cell_type': 'markdown', 'source': '\\nAs with classical machine learning techniques, it is required in deep learning to split off a hold-out *test set* which remains untouched during model training and tuning. It is later used to evaluate the model performance. On top, we will also split off an additional *validation set*, the reason of which will hopefully become clearer later in this lesson.\\n\\nTo make our lives a bit easier, we employ a trick to create these 3 datasets, `training set`, `test set` and `validation set`, by calling the `train_test_split` method of `scikit-learn` twice.\\n\\nFirst we create the training set and leave the remainder of 30 % of the data to the two hold-out sets.\\n', 'metadata': {}}\n",
      "32: {'id': '4fb6b379', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'from sklearn.model_selection import train_test_split\\n\\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_data, y_data, test_size=0.3, random_state=0)', 'outputs': []}\n",
      "33: {'id': 'b46183f8', 'cell_type': 'markdown', 'source': '\\nNow we split the 30 % of the data in two equal sized parts.\\n', 'metadata': {}}\n",
      "34: {'id': 'e3918d7e', 'cell_type': 'code', 'metadata': {}, 'execution_count': None, 'source': 'X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0)', 'outputs': []}\n",
      "35: {'id': '24fd5706', 'cell_type': 'markdown', 'source': '\\nSetting the `random_state` to `0` is a short-hand at this point. Note however, that changing this seed of the pseudo-random number generator will also change the composition of your data sets. For the sake of reproducibility, this is one example of a parameters that should not change at all.\\n\\n\\n', 'metadata': {}}\n",
      "36: {'id': '23ec4ddf', 'cell_type': 'markdown', 'source': '## 4. Choose a pretrained model or start building architecture from scratch', 'metadata': {}}\n",
      "37: {'id': 'd96b9adb', 'cell_type': 'markdown', 'source': '### Regression and classification', 'metadata': {}}\n",
      "38: {'id': '402d8726', 'cell_type': 'markdown', 'source': '\\nIn episode 2 we trained a dense neural network on a *classification task*. For this one hot encoding was used together with a `Categorical Crossentropy` loss function.\\nThis measured how close the distribution of the neural network outputs corresponds to the distribution of the three values in the one hot encoding.\\nNow we want to work on a *regression task*, thus not predicting a class label (or integer number) for a datapoint. In regression, we like to predict one (and sometimes many) values of a feature. This is typically a floating point number.\\n', 'metadata': {}}\n",
      "39: {'id': '05bfa0d2', 'cell_type': 'markdown', 'source': '### Challenge', 'metadata': {}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nbformat as nbf\n",
    "\n",
    "def rmd_to_notebook(rmd_file, notebook_file, base_image_url):\n",
    "    # Read the R Markdown file\n",
    "    with open(rmd_file, 'r') as file:\n",
    "        rmd_lines = file.readlines()\n",
    "    \n",
    "    # Preview the first 100 lines of the R Markdown file\n",
    "    print(\"Preview of the first 100 lines of the R Markdown file:\")\n",
    "    for i, line in enumerate(rmd_lines[:100]):\n",
    "        print(f\"{i+1}: {line}\", end='')\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    rmd_content = ''.join(rmd_lines)\n",
    "    \n",
    "    # Initialize a new notebook\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    cells = []\n",
    "\n",
    "    # Extract the title from the YAML front matter\n",
    "    # YAML front matter starts and ends with '---' and contains title information\n",
    "    title_match = re.search(r'^---\\s*title:\\s*\"(.*?)\".*?---', rmd_content, flags=re.DOTALL | re.MULTILINE)\n",
    "    if title_match:\n",
    "        title = title_match.group(1)\n",
    "        cells.append(nbf.v4.new_markdown_cell(f\"# {title}\"))\n",
    "    \n",
    "    # Remove the YAML front matter\n",
    "    rmd_content = re.sub(r'^---.*?---', '', rmd_content, flags=re.DOTALL | re.MULTILINE)\n",
    "    \n",
    "    # Remove instructor notes and solutions\n",
    "    # These are wrapped with ::: instructor or ::: solution\n",
    "    rmd_content = re.sub(r':::\\s*(instructor|solution)\\s*.*?:::', '', rmd_content, flags=re.DOTALL)\n",
    "\n",
    "    # Replace local image paths with GitHub raw URLs and handle alt text\n",
    "    def replace_image_path(match):\n",
    "        # Extract the alt text and image path from the match\n",
    "        alt_text = match.group(3)\n",
    "        image_path = match.group(2)\n",
    "        # Construct the web image URL using the base image URL and the extracted image path\n",
    "        web_image_url = f\"{base_image_url}/{image_path.split('/')[-1]}\"\n",
    "        # Create a Markdown header for the alt text\n",
    "        alt_header = f\"#### {alt_text.strip('alt=')}\"\n",
    "        # Return the Markdown image link followed by the alt text header\n",
    "        return f\"![{alt_text}]({web_image_url})\\n\\n{alt_header}\"\n",
    "\n",
    "    # Update the base image URL to point to the raw content\n",
    "    base_image_url = base_image_url.replace('/tree/', '/raw/')\n",
    "    \n",
    "    # Replace image links in the R Markdown content\n",
    "    rmd_content = re.sub(r'!\\[(.*?)\\]\\((.*?)\\)\\{(.*?)\\}', replace_image_path, rmd_content)\n",
    "\n",
    "    # Define regex patterns\n",
    "    # Header pattern matches Markdown headers (e.g., # Header, ## Subheader)\n",
    "    header_pattern = re.compile(r'^(#+) (.*)', re.MULTILINE)\n",
    "    # Block start pattern matches lines starting with ':::', optionally followed by a word\n",
    "    block_start_pattern = re.compile(r'^::+(\\s*\\w+)?')\n",
    "    # Block end pattern matches lines starting with ':::'\n",
    "    block_end_pattern = re.compile(r'^::+')\n",
    "    \n",
    "    # Split the content into lines\n",
    "    lines = rmd_content.split('\\n')\n",
    "    \n",
    "    code_buffer = []\n",
    "    is_in_code_block = False\n",
    "    text_buffer = []\n",
    "    is_in_special_block = False\n",
    "    special_block_type = \"\"\n",
    "\n",
    "    def process_buffer(buffer):\n",
    "        if buffer:\n",
    "            cells.append(nbf.v4.new_markdown_cell('\\n'.join(buffer)))\n",
    "            buffer.clear()\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('```'):\n",
    "            # Toggle the code block state\n",
    "            is_in_code_block = not is_in_code_block\n",
    "            if is_in_code_block:\n",
    "                # Process text buffer when entering a code block\n",
    "                process_buffer(text_buffer)\n",
    "            else:\n",
    "                # Add code cell when exiting a code block\n",
    "                if code_buffer:\n",
    "                    cells.append(nbf.v4.new_code_cell('\\n'.join(code_buffer).strip()))\n",
    "                    code_buffer = []\n",
    "        elif is_in_code_block:\n",
    "            # Collect lines within a code block\n",
    "            code_buffer.append(line)\n",
    "        else:\n",
    "            header_match = header_pattern.match(line)\n",
    "            block_start_match = block_start_pattern.match(line)\n",
    "            block_end_match = block_end_pattern.match(line)\n",
    "\n",
    "            if header_match:\n",
    "                # Process text buffer when encountering a header\n",
    "                process_buffer(text_buffer)\n",
    "                header_level = len(header_match.group(1))\n",
    "                header_text = header_match.group(2)\n",
    "                cells.append(nbf.v4.new_markdown_cell(f\"{'#' * header_level} {header_text}\"))\n",
    "            elif block_start_match:\n",
    "                # Process text buffer when encountering a block start\n",
    "                process_buffer(text_buffer)\n",
    "                if block_start_match.group(1):\n",
    "                    is_in_special_block = True\n",
    "                    special_block_type = block_start_match.group(1).strip().capitalize()\n",
    "                    text_buffer.append(f\"### {special_block_type}\")\n",
    "            elif block_end_match and is_in_special_block:\n",
    "                # Process text buffer when encountering a block end\n",
    "                process_buffer(text_buffer)\n",
    "                is_in_special_block = False\n",
    "            elif not block_end_match:  # Ignore lines with only colons\n",
    "                text_buffer.append(line)\n",
    "    \n",
    "    # Process any remaining text in the buffer\n",
    "    process_buffer(text_buffer)\n",
    "\n",
    "    # Remove empty cells\n",
    "    cells = [cell for cell in cells if cell['source'].strip()]\n",
    "\n",
    "    # Print the cells\n",
    "    print(\"Cells in the notebook:\")\n",
    "    for cell_ct, cell in enumerate(cells[:40]):\n",
    "        print(f\"{cell_ct}: {cell}\")\n",
    "        \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Add cells to the notebook\n",
    "    nb['cells'] = cells\n",
    "    \n",
    "    # Write the notebook to a file\n",
    "    with open(notebook_file, 'w') as file:\n",
    "        nbf.write(nb, file)\n",
    "\n",
    "# Use the function to convert an R Markdown file to a Jupyter notebook\n",
    "input_dir = '../episodes/' \n",
    "base_image_url = 'https://github.com/carpentries-incubator/deep-learning-intro/raw/main/episodes/fig'# file_in = '2-keras'\n",
    "file_in = '3-monitor-the-model'\n",
    "# file_in = '4-advanced-layer-types'\n",
    "# file_in = '5-transfer-learning'\n",
    "rmd_to_notebook(input_dir+file_in+'.Rmd', input_dir+file_in+'.ipynb', base_image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddde69e-a8da-4ddc-a3b3-f1f371f01a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
