<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Trustworthy AI: Explainability, Bias, and Fairness: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Trustworthy AI: Explainability, Bias, and Fairness
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Trustworthy AI: Explainability, Bias, and Fairness
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="../aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Trustworthy AI: Explainability, Bias, and Fairness
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="0-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="1-preparing-to-train.html">2. Preparing to train a model</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="2-model-fitting.html">3. Scientific Validity in the Modeling Process</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="3-model-eval.html">4. Model evaluation and fairness</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="4-explainability.html">5. Explainability</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="5-releasing-a-model.html">6. Releasing a model</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-0-introduction"><p>Content from <a href="0-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2023-12-14 |
        
        <a href="https://github.com/annapmeyer/fair-explainable-ml/edit/main/episodes/0-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you write a lesson using Markdown and
<a href="https://carpentries.github.io/sandpaper/" class="external-link">sandpaper</a>?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain how to use markdown with The Carpentries Workbench</li>
<li>Demonstrate how to include pieces of code, figures, and nested
challenge blocks</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="introduction"><h2 class="section-heading">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>This is a lesson created via The Carpentries Workbench. It is written
in <a href="https://pandoc.org/MANUAL.txt" class="external-link">Pandoc-flavored Markdown</a>
for static files and <a href="https://rmarkdown.rstudio.com/" class="external-link">R
Markdown</a> for dynamic files that can render code into output. Please
refer to the <a href="https://carpentries.github.io/sandpaper-docs/" class="external-link">Introduction to The
Carpentries Workbench</a> for full documentation.</p>
<p>What you need to know is that there are three sections required for a
valid Carpentries lesson:</p>
<ol style="list-style-type: decimal">
<li>
<code>questions</code> are displayed at the beginning of the episode
to prime the learner for the content.</li>
<li>
<code>objectives</code> are the learning objectives for an episode
displayed with the questions.</li>
<li>
<code>keypoints</code> are displayed at the end of the episode to
reinforce the objectives.</li>
</ol>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div>Instructor Note</h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<div id="challenge-1-can-you-do-it" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1-can-you-do-it" class="callout-inner">
<h3 class="callout-title">Challenge 1: Can you do it?<a class="anchor" aria-label="anchor" href="#challenge-1-can-you-do-it"></a>
</h3>
<div class="callout-content">
<p>What is the output of this command?</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">paste</span><span class="op">(</span><span class="st">"This"</span>, <span class="st">"new"</span>, <span class="st">"lesson"</span>, <span class="st">"looks"</span>, <span class="st">"good"</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Output</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "This new lesson looks good"</code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge-2-how-do-you-nest-solutions-within-challenge-blocks" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2-how-do-you-nest-solutions-within-challenge-blocks" class="callout-inner">
<h3 class="callout-title">Challenge 2: how do you nest solutions within
challenge blocks?<a class="anchor" aria-label="anchor" href="#challenge-2-how-do-you-nest-solutions-within-challenge-blocks"></a>
</h3>
<div class="callout-content">

</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>You can add a line with at least three colons and a
<code>solution</code> tag.</p>
</div>
</div>
</div>
</div>
</section><section id="figures"><h2 class="section-heading">Figures<a class="anchor" aria-label="anchor" href="#figures"></a>
</h2>
<hr class="half-width">
<p>You can use standard markdown for static figures with the following
syntax:</p>
<p><code>![optional caption that appears below the figure](figure url){alt='alt text for accessibility purposes'}</code></p>
<figure><img src="https://raw.githubusercontent.com/carpentries/logo/master/Badge_Carpentries.svg" alt="Blue Carpentries hex person logo with no text." class="figure mx-auto d-block"><div class="figcaption">You belong in The Carpentries!</div>
</figure><div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Callout sections can highlight information.</p>
<p>They are sometimes used to emphasise particularly important points
but are also used in some lessons to present “asides”: content that is
not central to the narrative of the lesson, e.g. by providing the answer
to a commonly-asked question.</p>
</div>
</div>
</div>
</section><section id="math"><h2 class="section-heading">Math<a class="anchor" aria-label="anchor" href="#math"></a>
</h2>
<hr class="half-width">
<p>One of our episodes contains <span class="math inline">\(\LaTeX\)</span> equations when describing how to
create dynamic reports with {knitr}, so we now use mathjax to describe
this:</p>
<p><code>$\alpha = \dfrac{1}{(1 - \beta)^2}$</code> becomes: <span class="math inline">\(\alpha = \dfrac{1}{(1 - \beta)^2}\)</span></p>
<p>Cool, right?</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Use <code>.md</code> files for episodes when you want static
content</li>
<li>Use <code>.Rmd</code> files for episodes when you need to generate
output</li>
<li>Run <code>sandpaper::check_lesson()</code> to identify any issues
with your lesson</li>
<li>Run <code>sandpaper::build_lesson()</code> to preview your lesson
locally</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-1-preparing-to-train"><p>Content from <a href="1-preparing-to-train.html">Preparing to train a model</a></p>
<hr>
<p>Last updated on 2023-12-14 |
        
        <a href="https://github.com/annapmeyer/fair-explainable-ml/edit/main/episodes/1-preparing-to-train.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 0 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>For what prediction tasks is machine learning an appropriate
tool?</li>
<li>How can inappropriate target variable choice lead to suboptimal
outcomes in a machine learning pipeline?</li>
<li>What is “biased” training data, and where does this bias come
from?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Judge what tasks are appropriate for machine learning</li>
<li>Understand why the choice of prediction task / target variable is
important.</li>
<li>Describe how bias can appear in training data.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Some tasks are not appropriate for machine learning due to ethical
concerns.</li>
<li>Machine learning tasks should have a valid prediction target that
maps clearly to the real-world goal.</li>
<li>Training data can be biased due to societal inequities, errors in
the data collection process, and lack of attention to careful sampling
practices.</li>
</ul>
</div>
</div>
</div></section><section id="aio-2-model-fitting"><p>Content from <a href="2-model-fitting.html">Scientific Validity in the Modeling Process</a></p>
<hr>
<p>Last updated on 2023-12-14 |
        
        <a href="https://github.com/annapmeyer/fair-explainable-ml/edit/main/episodes/2-model-fitting.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 0 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What impact does overfitting and underfitting have on model
performance?</li>
<li>What is data leakage?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Implement at least two types of machine learning models in
Python.</li>
<li>Describe the risks of, identify, and understand mitigation steps for
overfitting and underfitting.</li>
<li>Understand why data leakage is harmful to scientific validity and
how it can appear in machine learning pipelines.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Overfitting is characterized by worse performance on the test set
than on the train set and can be fixed by switching to a simpler model
architecture or by adding regularization.</li>
<li>Underfitting is characterized by poor performance on both the
training and test datasets. It can be fixed by collecting more training
data, switching to a more complex model architecture, or improving
feature quality.</li>
<li>Data leakage occurs when the model has access to the test data
during training and results in overconfidence in the model’s
performance.</li>
</ul>
</div>
</div>
</div></section><section id="aio-3-model-eval"><p>Content from <a href="3-model-eval.html">Model evaluation and fairness</a></p>
<hr>
<p>Last updated on 2024-03-21 |
        
        <a href="https://github.com/annapmeyer/fair-explainable-ml/edit/main/episodes/3-model-eval.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 0 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do we define fairness and bias in machine learning
outcomes?</li>
<li>What types of bias and unfairness can occur in generative AI?</li>
<li>How can we improve the fairness of machine learning models?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Reason about model performance through standard evaluation
metrics.</li>
<li>Understand and distinguish between various notions of fairness in
machine learning.</li>
<li>Describe and implement two different ways of modifying the machine
learning modeling process to improve the fairness of a model.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="accuracy-metrics"><h2 class="section-heading">Accuracy metrics<a class="anchor" aria-label="anchor" href="#accuracy-metrics"></a>
</h2>
<hr class="half-width">
<p>Stakeholders often want to know the accuracy of a machine learning
model – what percent of predictions are correct? Accuracy can be
decomposed into further metrics: e.g., in a binary prediction setting,
recall (the fraction of positive samples that are classified correctly)
and precision (the fraction of samples classified as positive that
actually are positive) are commonly-used metrics.</p>
<p>Suppose we have a model that performs binary classification (+, -) on
a test dataset of 1000 samples (let <span class="math inline">\(n\)</span>=1000). A <em>confusion matrix</em>
defines how many predictions we make in each of four quadrants: true
positive with positive prediction (++), true positive with negative
prediction (+-), true negative with positive prediction (-+), and true
negative with negative prediction (–).</p>
<table class="table">
<thead><tr class="header">
<th></th>
<th>True +</th>
<th>True -</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Predicted +</td>
<td>300</td>
<td>80</td>
</tr>
<tr class="even">
<td>Predicted -</td>
<td>25</td>
<td>595</td>
</tr>
</tbody>
</table>
<p>So, for instance, 80 samples have a true class of + but get predicted
as members of -.</p>
<p>We can compute the following metrics:</p>
<ul>
<li>Accuracy: What fraction of predictions are correct?
<ul>
<li>(300 + 595) / 100 = 0.895</li>
<li>Accuracy is 89.5%</li>
</ul>
</li>
<li>Precision: What fraction of predicted positives are true positives?
<ul>
<li>300 / (300 + 80) = 0.789</li>
<li>Precision is 78.9%</li>
</ul>
</li>
<li>Recall: What fraction of true positives are classified as positive?
<ul>
<li>300 / (300 + 25) = 0.923</li>
<li>Recall is 92.3%</li>
</ul>
</li>
</ul>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>TODO we’ve discussed binary classification but for other types of
tasks there are different metrics. E.g., top-k accuracy for multi-class
problems, ROC for regression tasks</p>
<p>TODO also discuss F1 score here?</p>
</div>
</div>
</div>
<div id="what-accuracy-metric-to-use" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="what-accuracy-metric-to-use" class="callout-inner">
<h3 class="callout-title">What accuracy metric to use?<a class="anchor" aria-label="anchor" href="#what-accuracy-metric-to-use"></a>
</h3>
<div class="callout-content">
<p>Different accuracy metrics may be more relevant in different
situations. Discuss with a partner or small groups whether precision,
recall, or some combination of the two is most relevant in the following
prediction tasks:</p>
<ol style="list-style-type: decimal">
<li>Deciding what patients are high risk for a disease and who should
get additional low-cost screening.</li>
<li>Deciding what patients are high risk for a disease and should start
taking medication to lower the disease risk. The medication is expensive
and can have unpleasant side effects.</li>
<li><strong>TODO</strong></li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li><p>It is best if all patients who need the screening get it, and
there is little downside for doing screenings unnecessarily because the
screening costs are low. Thus, a high recall score is optimal.</p></li>
<li><p>Given the costs and side effects of the medicine, we do not want
patients not at risk for the disease to take the medication. So, a high
precision score is ideal.</p></li>
<li><p><strong>TODO</strong></p></li>
</ol>
</div>
</div>
</div>
</div>
</section><section id="how-do-we-measure-fairness"><h2 class="section-heading">How do we measure fairness?<a class="anchor" aria-label="anchor" href="#how-do-we-measure-fairness"></a>
</h2>
<hr class="half-width">
<p>What does it mean for a machine learning model to be fair or
unbiased? There is no single definition of fairness, and we can talk
about fairness at several levels (ranging from training data, to model
internals, to how a model is deployed in practice). Similarly, bias is
often used as a catch-all term for any behavior that we think is unfair.
Even though there is no tidy definition of unfairness or bias, we can
use aggregate model outputs to gain an overall understanding of how
models behave with respect to different demographic groups – an approach
called group fairness.</p>
<p>In general, if there are no differences between groups in the real
world (e.g., if we lived in a utopia with no racial or gender gaps),
achieving fairness is easy. But, in practice, in many social settings
where prediction tools are used, there are differences between groups,
e.g., due to historical and current discrimination.</p>
<p>For instance, in a loan prediction setting in the United States, the
average white applicant may be better positioned to repay a loan than
the average Black applicant due to differences in generational wealth,
education opportunities, and other factors stemming from anti-Black
racism. Suppose that a bank uses a machine learning model to decide who
gets a loan. Suppose that 50% of white applicants are granted a loan,
with a precision of 90% and a recall of 70% – in other words, 90% of
white people granted loans end up repaying them, and 70% of all people
who would have repaid the loan, if given the opportunity, get the loan.
Consider the following scenarios:</p>
<ul>
<li>(Demographic parity) We give loans to 50% of Black applicants in a
way that maximizes overall accuracy</li>
<li>(Equalized odds) We give loans to X% of Black applicants, where X is
chosen to maximize accuracy subject to keeping precision equal to
90%.</li>
<li>(Group level calibration) We give loans to X% of Black applicants,
where X is chosen to maximize accuracy while keeping recall equal to
70%.</li>
</ul>
<p>There are <em>many</em> notions of statistical group fairness, but
most boil down to one of the three above options: demographic parity,
equalized odds, and group-level calibration.</p>
<p><strong>TODO</strong> need example here, case study</p>
<p><strong>TODO</strong> need discussion of individual fairness
(especially if we keep the challenge below)</p>
<div id="matching-fairness-terminology-with-definitions" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="matching-fairness-terminology-with-definitions" class="callout-inner">
<h3 class="callout-title">Matching fairness terminology with
definitions<a class="anchor" aria-label="anchor" href="#matching-fairness-terminology-with-definitions"></a>
</h3>
<div class="callout-content">
<p>Match the following types of formal fairness with their definitions.
(A) Individual fairness, (B) Equalized odds, (C) Demographic parity, and
(D) Group-level calibration</p>
<ol style="list-style-type: decimal">
<li>The model is equally accurate across all demographic groups.</li>
<li>Different demographic groups have the same true positive rates and
false positive rates.</li>
<li>Similar people are treated similarly.</li>
<li>People from different demographic groups receive each outcome at the
same rate.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>A - 3, B - 2, C - 4, D - 1</p>
</div>
</div>
</div>
</div>
<p>But some types of unfairness cannot be directly measured by
group-level statistical data. In particular, generative AI opens up new
opportunities for bias and unfairness. Bias can occur through
representational harms (e.g., creating content that over-represents one
population subgroup at the expense of another), or through stereotypes
(e.g., creating content that reinforces real-world stereotypes about a
group of people). We’ll discuss some specific examples of bias in
generative models next.</p>
</section><section id="fairness-in-generative-ai"><h2 class="section-heading">Fairness in generative AI<a class="anchor" aria-label="anchor" href="#fairness-in-generative-ai"></a>
</h2>
<hr class="half-width">
<p>Generative models learn from statistical patterns in real-world data.
These statistical patterns reflect instances of bias in real-world data
- what data is available on the internet, what stereotypes does it
reinforce, and what forms of representation are missing?</p>
<div class="section level3">
<h3 id="natural-language">Natural language<a class="anchor" aria-label="anchor" href="#natural-language"></a>
</h3>
<p>One set of social stereotypes that large AI models can learn is
gender based. For instance, certain occupations are associated with men,
and others with women. For instance, in the U.S., doctors are
historically and stereotypically usually men.</p>
<p>In 2016, Caliskan et al. <a href="https://www.fatml.org/schedule/2016/presentation/semantics-derived-automatically-language-corpora" class="external-link">showed
that machine translation systems exhibit gender bias</a>, for instance,
by reverting to stereotypical gendered pronouns in ambiguous
translations, like in Turkish – a language without gendered pronouns –
to English.</p>
<p>In response, Google <a href="https://blog.research.google/2018/12/providing-gender-specific-translations.html" class="external-link">tweaked
their translator algorithms</a> to identify and correct for gender
stereotypes in Turkish and several other widely-spoken languages. So
when we repeat a similar experiment today, we get the following
output:</p>
<figure><img src="https://raw.githubusercontent.com/annapmeyer/fair-explainable-ml/main/images/e4-turkish-nlp-stereotypes.png" alt='Screenshot of Google Translate output. The English sentence "The doctor is on her lunch break" is translated to Turkish, and then the Turkish output is translated back to English as either "The doctor is on his lunch break" or "The doctor is on his lunch break".' class="figure mx-auto d-block"><div class="figcaption">Turkish Google Translate example (screenshot
from 1/9/2024)</div>
</figure><p>But for other, less widely-spoken languages, the original problem
persists:</p>
<figure><img src="https://raw.githubusercontent.com/annapmeyer/fair-explainable-ml/main/images/e4-norwegian-nlp-stereotypes.png" alt='Screenshot of Google Translate output. The English sentence "The doctor is on her lunch break" is translated to Norwegian, and then the Norwegian output is translated back to English as "The doctor is on his lunch break".' class="figure mx-auto d-block"><div class="figcaption">Norwegian Google Translate example (screenshot
from 1/9/2024)</div>
</figure><p>We’re not trying to slander Google Translate here – the translation,
without additional context, is ambiguous. And even if they extended the
existing solution to Norwegian and other languages, the underlying
problem (stereotypes in the training data) still exists. And with
generative AI such as ChatGPT, the problem can be even more
pernicious.</p>
<div id="red-teaming-large-language-models" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="red-teaming-large-language-models" class="callout-inner">
<h3 class="callout-title">Red-teaming large language models<a class="anchor" aria-label="anchor" href="#red-teaming-large-language-models"></a>
</h3>
<div class="callout-content">
<p>In cybersecurity, “red-teaming” is when well-intentioned people think
like a hacker in order to make a system safer. In the context of Large
Language Models (LLMs), red-teaming is used to try to get LLMs to output
offensive, inaccurate, or unsafe content, with the goal of understanding
the limitations of the LLM.</p>
<p>Try out red-teaming with ChatGPT or another LLM. Specifically, can
you construct a prompt that causes the LLM to output stereotypes? Here
are some example prompts, but feel free to get creative!</p>
<ul>
<li><p>“Tell me a story about a doctor” (or other profession with
gender)</p></li>
<li><p>If you speak a language other than English, how does are
ambiguous gendered pronouns handled? For instance, try the prompt
“Translate ‘The doctor is here’ to Spanish”. Is a masculine or feminine
pronoun used for the doctor in Spanish?</p></li>
</ul>
<p>If you use LLMs in your research, consider whether any of these
issues are likely to be present for your use cases. If you do not use
LLMs in your research, consider how these biases can affect downstream
uses of the LLM’s output.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Most publicly-available LLM providers set up guardrails to avoid
propagating biases present in their training data. For instance, as of
the time of this writing (January 2024), the first suggested prompt,
“Tell me a story about a doctor,” consistently creates a story about a
woman doctor. Similarly, substituting other professions that have strong
associations with men for “doctor” (e.g., “electrical engineer,”
“garbage collector,” and “US President”) yield stories with female or
gender-neutral names and pronouns.</p>
</div>
</div>
</div>
</div>
<div id="discussing-other-fairness-issues" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussing-other-fairness-issues" class="callout-inner">
<h3 class="callout-title">Discussing other fairness issues<a class="anchor" aria-label="anchor" href="#discussing-other-fairness-issues"></a>
</h3>
<div class="callout-content">
<p>If you use LLMs in your research, consider whether any of these
issues are likely to be present for your use cases. Share your thoughts
in small groups with other workshop participants.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-generation">Image generation<a class="anchor" aria-label="anchor" href="#image-generation"></a>
</h3>
<p>The same problems that language modeling face also affect image
generation. Consider, for instance, Melon et al. <a href="https://arxiv.org/pdf/2003.03808.pdf" class="external-link">developed an algorithm
called Pulse</a> that can convert blurry images to higher resolution.
But, biases were quickly unearthed and <a href="https://twitter.com/Chicken3gg/status/1274314622447820801?s=20&amp;t=_oORPJBJRaBW_J0zresFJQ" class="external-link">shared
via social media</a>.</p>
<div id="discussion5" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion5"></a>
</h3>
<div class="callout-content">
<p>Who is shown in this blurred picture? <img src="https://raw.githubusercontent.com/annapmeyer/fair-explainable-ml/main/images/e4-obama.png" alt="blurry image of Barack Obama" class="figure"></p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>While the picture is of Barack Obama, the upsampled image shows a
white face. <img src="https://raw.githubusercontent.com/annapmeyer/fair-explainable-ml/main/images/e4-obama-upsampled.png" alt="Unblurred version of the pixelated picture of Obama. Instead of showing Obama, it shows a white man." class="figure"></p>
<p>You can <a href="https://colab.research.google.com/github/tg-bomze/Face-Depixelizer/blob/master/Face_Depixelizer_Eng.ipynb#scrollTo=fU0aGtD4Nl4W" class="external-link">try
the model here</a>.</p>
</div>
</div>
</div>
</div>
<p>Menon and colleagues subsequently updated their paper to discuss this
issue of bias. They assert that the problems inherent in the PULSE model
are largely a result of the <a href="https://arxiv.org/abs/1812.04948" class="external-link">underlying StyleGAN model</a>,
which they had used in their work.</p>
<blockquote>
<p>Overall, it seems that sampling from StyleGAN yields white faces much
more frequently than faces of people of color … This bias extends to any
downstream application of StyleGAN, including the implementation of
PULSE using StyleGAN.</p>
<p>…</p>
<p>Results indicate a racial bias among the generated pictures, with
close to three-fourths (72.6%) of the pictures representing White
people. Asian (13.8%) and Black (10.1%) are considerably less frequent,
while Indians represent only a minor fraction of the pictures
(3.4%).</p>
</blockquote>
<p>These remarks get at a central issue: biases in any building block of
a system (data, base models, etc.) get propagated forwards. In
generative AI, such as text-to-image systems, this can result in
representational harms, <a href="https://arxiv.org/pdf/2211.03759.pdf" class="external-link">as documented by Bianchi et
al.</a> Fixing these issues of bias is still an active area of research.
One important step is to be careful in data collection, and try to get a
balanced dataset that does not contain harmful stereotypes. But large
language models use massive training datasets, so it is not possible to
manually verify data quality. Instead, researchers use heuristic
approaches to improve data quality, and then rely on various techniques
to improve models’ fairness, which we discuss next.</p>
</div>
</section><section id="improving-fairness-of-models"><h2 class="section-heading">Improving fairness of models<a class="anchor" aria-label="anchor" href="#improving-fairness-of-models"></a>
</h2>
<hr class="half-width">
<p>Model developers frequently try to improve the fairness of there
model by intervening at one of three stages: pre-processing,
in-processing, or post-processing. We’ll cover techniques within each of
these paradigms in turn.</p>
<p>We start, though, by discussing why removing the sensitive
attribute(s) is not sufficient. Consider the task of deciding which loan
applicants are funded. Suppose we are concerned with racial bias in the
model outputs. If we remove race from the set of attributes available to
the model, the model cannot make <em>overly</em> racist decisions.
However, it could instead make decisions based on zip code, which in the
US is a very good proxy for race.</p>
<p>Can we simply remove all proxy variables? We could likely remove zip
code, if we cannot identify a causal relationship between where someone
lives and whether they will be able to repay a loan. But what about an
attribute like educational achievement? Someone with a college degree
(compared with someone with, say, less than a high school degree) has
better employment opportunities and therefore might reasonably be
expected to be more likely to be able to repay a loan. However,
educational attainment is still a proxy for race in the United States
due to historical (and ongoing) discrimination.</p>
<p><strong>Pre-processing</strong> generally modifies the dataset used
for learning. Techniques in this category include</p>
<ul>
<li><p>Oversampling/undersampling: instead of training a machine
learning model on all of the data, <em>undersample</em> the majority
class by removing some of the majority class samples from the dataset in
order to have a more balanced dataset. Alternatively,
<em>oversample</em> the minority class by duplicating samples belonging
to this group.</p></li>
<li><p>Data augmentation: the number of samples from minority groups may
be increased by generating synthetic data with a generative adversarial
network (GAN). We won’t cover this method in this workshop (using a GAN
can be more computationally expensive than other techniques). If you’re
interested, you can learn more about this method from the paper <a href="https://link.springer.com/chapter/10.1007/978-3-030-58542-6_23" class="external-link">Inclusive
GAN: Improving Data and Minority Coverage in Generative
Models</a>.</p></li>
<li><p>Changing feature representations: various techniques have been
proposed to increase fairness by removing unfairness from the data
directly. To do so, the data is converted into an alternate
representation so that differences between demographic groups are
minimized, yet enough information is maintained in order to be able to
learn a model that performs well. An advantage of this method is that it
is model-agnostic, however, a challenge is it reduces the
interpretability of interpretable models and makes post-hoc
explainability less meaningful for black-box models.</p></li>
</ul>
<div id="pros-and-cons-of-preprocessing-options" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="pros-and-cons-of-preprocessing-options" class="callout-inner">
<h3 class="callout-title">Pros and cons of preprocessing options<a class="anchor" aria-label="anchor" href="#pros-and-cons-of-preprocessing-options"></a>
</h3>
<div class="callout-content">
<p>Discuss what you think the pros and cons of the different
pre-processing options are. What techniques might work better in
different settings?</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>A downside of oversampling is that it may violate statistical
assumptions about independence of samples. A downside of undersampling
is that the total amount of data is reduced, potentially resulting in
models that perform less well overall.</p>
<p>A downside of using GANs to generate additional data is that this
process may be expensive and require higher levels of ML expertise.</p>
<p>A challenge with all techniques is that if there is not sufficient
data from minority groups, it may be hard to achieve good performance on
the groups without simply collecting more or higher-quality data.</p>
</div>
</div>
</div>
</div>
<p><strong>In-processing</strong> modifies the learning algorithm.</p>
<ul>
<li><p>Reweighting samples: many machine learning models allow for
reweighting individual samples, i.e., indicating that misclassifying
certain, rarer, samples should be penalized more severely in the loss
function. In the code example, we show how to reweight samples using
AIF360’s <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html" class="external-link">Reweighting</a>
function.</p></li>
<li><p>Incorporating fairness into the loss function: reweighting
explicitly instructs the loss function to penalize the misclassification
of certain samples more harshly. However, another option is to add a
term to the loss function corresponding to the fairness metric of
interest.</p></li>
</ul>
<p><strong>Post-processing</strong> modifies an existing model to
increase its fairness.</p>
<ul>
<li>different thresholds</li>
</ul>
<div id="other-fairness-interventions" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="other-fairness-interventions" class="callout-inner">
<h3 class="callout-title">Other fairness interventions<a class="anchor" aria-label="anchor" href="#other-fairness-interventions"></a>
</h3>
<div class="callout-content">
<p>Computer scientists have proposed many interventions to improve the
fairness of machine learning models. A partial list is available (TODO
FIND RESOURCE). Visit the list and read about one of the methods. When
would using that method be beneficial for fairness? How does it compare
to the techniques we talked about above?</p>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" data-bs-parent="#accordionSolution6" aria-labelledby="headingSolution6">
<div class="accordion-body">
<p>TODO (discuss one or two?)</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>It’s important to consider many dimensions of model performance: a
single accuracy score is not sufficient.</li>
<li>There is no single definition of “fair machine learning”: different
notions of fairness are appropriate in different contexts.</li>
<li>Representational harms and stereotypes can be perpetuated by
generative AI.</li>
<li>The fairness of a model can be improved by using techniques like
data reweighting and model postprocessing.</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-4-explainability"><p>Content from <a href="4-explainability.html">Explainability</a></p>
<hr>
<p>Last updated on 2023-12-14 |
        
        <a href="https://github.com/annapmeyer/fair-explainable-ml/edit/main/episodes/4-explainability.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 0 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is model interpretability? When do we need models to be
interpretable?</li>
<li>What are some model interpretability techniques?</li>
<li>When are model explainability techniques not sufficient for
understanding model behavior?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Compare and contrast different interpretability techniques.</li>
<li>Explain feature importance.</li>
<li>Articulate limitations of explainable machine learning.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>TODO</li>
</ul>
</div>
</div>
</div></section><section id="aio-5-releasing-a-model"><p>Content from <a href="5-releasing-a-model.html">Releasing a model</a></p>
<hr>
<p>Last updated on 2023-12-14 |
        
        <a href="https://github.com/annapmeyer/fair-explainable-ml/edit/main/episodes/5-releasing-a-model.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 0 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is distribution shift? How can we know if distribution shift
has occured?</li>
<li>What is a model card?</li>
<li>How do I share a model so that others may use it?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand distribution shift and its implications.</li>
<li>Apply model-sharing best practices through using model cards.</li>
<li>Understand the technical and communication norms around sharing
models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Distribution shift is common. It can be caused by temporal shifts
(i.e., using old training data) or by applying a model to new
populations.</li>
<li>Distribution shift can be addressed by TODO</li>
<li>Model cards are the standard technique for communicating information
about how machine learning systems were trained and how they should and
should not be used.</li>
<li>Models can be shared and reused by doing TODO</li>
</ul>
</div>
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/annapmeyer/fair-explainable-ml/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/annapmeyer/fair-explainable-ml/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/annapmeyer/fair-explainable-ml/" class="external-link">Source</a></p>
				<p><a href="https://github.com/annapmeyer/fair-explainable-ml/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:apmeyer4@wisc.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://annapmeyer.github.io/fair-explainable-ml/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "fairness, explainability, fair machine learning, interpretable machine learning, xai, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://annapmeyer.github.io/fair-explainable-ml/instructor/aio.html",
  "identifier": "https://annapmeyer.github.io/fair-explainable-ml/instructor/aio.html",
  "dateCreated": "2023-12-05",
  "dateModified": "2024-04-16",
  "datePublished": "2024-04-16"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

